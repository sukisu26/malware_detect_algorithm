'''
* trainAlgo: this program train a algorithm to detect malware from PE files
*
* Author: WanQi Su                                           
'''
# import useful tools
import numpy
import pandas
import pickle 
from matplotlib import pyplot
from xgboost import XGBClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import model_selection, tree
from sklearn.metrics import confusion_matrix
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib


# get data in data.csv (PE files whith labeled malicious or legitimate->supervised algorithm)
df1=pandas.read_csv('data.csv',sep='|')


# let legitimate be the y_value and the features be the x_value
# notice: Name--column 0, md5--column1, legitimate--colum56
df2=df1.drop(['Name','md5','legitimate'],axis='columns')
x_value=df2.values
y_value=df1['legitimate'].values
#print(df1.columns.get_loc("legitimate"))

print("Total number of features is: "+str(len(df2.columns)))

# use tree-based feature selection to find important features in the 54 features for a Classifier
# build a model
classifier=ExtraTreesClassifier()
classifier=classifier.fit(x_value,y_value)
model = SelectFromModel(classifier, prefit=True)

# deduct dimension of x_value
x_new=model.transform(x_value)
# number of features
num_of_feature=x_new.shape[1]
print(str(num_of_feature)+" features are important(in order): ")

# sort the important features  based on the improtance_rate
importances_rate = classifier.feature_importances_
# return the sorted index of features
indexSortedList = numpy.argsort(importances_rate)[::-1][:num_of_feature]

# put sorted features in features array instead of index in the indexSortedList
features=[]
for index in sorted(indexSortedList):
    features.append(df2.columns[index])


# print features in order
for index in range(num_of_feature):
    print(str(index+1)+". "+df2.columns[indexSortedList[index]]+" [%.5f]" %importances_rate[indexSortedList[index]])
# Plot important features
pyplot.title("Feature importances")
pyplot.bar(range(num_of_feature), importances_rate[indexSortedList],align="center")
pyplot.xticks(range(num_of_feature), indexSortedList)
pyplot.xlim([-1, num_of_feature])
pyplot.show()

# model comparision: comparing the prediction result

# split the data into two group: trained and test
x_train, x_test, y_train, y_test = model_selection.train_test_split(x_new, y_value ,test_size=0.25)

# we choose six algorithms for supervised data
Six_algorithms={
    "Decision_Trees: ": tree.DecisionTreeClassifier(),
    "XGBoost: ": XGBClassifier(),
    "Random_Forest": RandomForestClassifier(n_estimators=40),
    "K-nearest_Neighbor": KNeighborsClassifier(),
    "AdaBoost": AdaBoostClassifier(n_estimators=50),
    "GNB": GaussianNB()
}

Six_algorithms_accuracy={}
for name , model_classifier in Six_algorithms.items():
    fit_classfier=model_classifier.fit(x_train,y_train)
    classifier_accuracy=fit_classfier.score(x_test,y_test)
    print(name,classifier_accuracy)
    Six_algorithms_accuracy[name]=classifier_accuracy

# find the best model
max_value=0
best_algor=""
for name, value in Six_algorithms_accuracy.items():
    if max_value < value:
        max_value=value
        best_algor=name

print("The best algorithm is: %s with %f %% accuracy" %(best_algor, max_value*100))

#calculate the false postive rate and false negative rate to test the model
best_model_clf=Six_algorithms[best_algor]
regression=best_model_clf.predict(x_test)
matrix=confusion_matrix(y_test,regression)
false_positive=(matrix[0][1]/float(sum(matrix[0])))*100
false_negative=(matrix[1][0]/float(sum(matrix[1])))*100
print("The false positive rate is %f %%, and the false negative rate is %f %%" %(false_positive, false_negative))

# save the algorithm and the features for prediction later
print("save the algorithm and the feature list in classifier directory")
joblib.dump(Six_algorithms[best_algor], 'classifier/classifier.pkl')
open('classifier/features.pkl', 'w').write(pickle.dumps(features))
print('Saved')






